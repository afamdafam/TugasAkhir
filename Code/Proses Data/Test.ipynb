{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76840beb-7817-4a7d-bf43-e083da5ff414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import sys\n",
    "import urllib\n",
    "import glob\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Flatten, Input, Dropout, InputLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "237b2cb7-cb40-4098-af13-bd02b787ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic Keypoints : Pose Tubuh dan Tangan\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_drawing = mp.solutions.drawing_utils # Utilitas menggambar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f62eafa-bc84-4e97-acd4-53da1036fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_bbox = None\n",
    "glob_dist = None\n",
    "\n",
    "def find_body_centroid(landmarks, main_body):\n",
    "    if landmarks: ### jika landmark ditemukan\n",
    "        x_bodies = []\n",
    "        y_bodies = []\n",
    "        z_bodies = []\n",
    "        for i in main_body:\n",
    "            x_bodies.append(landmarks.landmark[i].x)\n",
    "            y_bodies.append(landmarks.landmark[i].y)\n",
    "            z_bodies.append(landmarks.landmark[i].z)\n",
    "        glob_bbox = [x_bodies, y_bodies, z_bodies]\n",
    "        return np.average(x_bodies), np.average(y_bodies), np.average(z_bodies)\n",
    "    else: ### jika landmark tidak ditemukan\n",
    "        return 0, 0, 0\n",
    "    \n",
    "def euclidean(a, b):\n",
    "    sum_sq = np.sum(np.square(a - b)) ## sumasi dari kedua titik\n",
    "    euclidean = np.sqrt(sum_sq) ## akar kuadrat dari sumasi\n",
    "\n",
    "    return euclidean\n",
    "\n",
    "def pixel_match(coordinates, w, h): # rescaling keypoints menyesuaikan resolusi frame\n",
    "    x = int(coordinates[0] * w)\n",
    "    y = int(coordinates[1] * h)\n",
    "    coordinates[0] = x\n",
    "    coordinates[1] = y\n",
    "\n",
    "    return coordinates\n",
    "\n",
    "def landmarks_data(landmarks, data, key):\n",
    "    radius = data[\"radius\"]\n",
    "    px_radius = data[\"px_radius\"]\n",
    "    centroid = data[\"centroid\"]\n",
    "    coordinates = []\n",
    "    centroid_x = centroid[0]\n",
    "    centroid_y = centroid[1]\n",
    "    original_h = data[\"image\"].shape[0]\n",
    "    original_w = data[\"image\"].shape[1]\n",
    "    px_centroid_x, px_centroid_y = pixel_match([centroid_x, centroid_y], original_w, original_h)\n",
    "    x = 0\n",
    "    if landmarks:\n",
    "        for i in landmarks.landmark:\n",
    "            horizontal = px_centroid_x - px_radius # selisih kanan/kiri\n",
    "            vertical = px_centroid_y - px_radius # selisih atas/bawah\n",
    "\n",
    "            px_x, px_y = pixel_match([i.x, i.y], original_w, original_h)\n",
    "            \n",
    "            if px_x >= 0: \n",
    "                normalized_px_x = abs(horizontal - px_x) \n",
    "            else:  \n",
    "                normalized_px_x = -abs(horizontal - px_x) \n",
    "            if px_y >= 0: \n",
    "                normalized_px_y = abs(vertical - px_y) \n",
    "            else: \n",
    "                normalized_px_y = -abs(vertical - px_y)\n",
    "\n",
    "            # data[\"img_pixel\"] = np.zeros([10*px_radius,10*px_radius,3],dtype=np.uint8)\n",
    "            # data[\"img_pixel\"].fill(155) # or img[:] = 155\n",
    "            \n",
    "            data[\"img_pixel\"] = data[\"image\"]\n",
    "            data[\"img_pixel\"] = cv2.circle(data[\"img_pixel\"], (px_x,px_y), radius=5, color=(66,66,245), thickness=3)\n",
    "            data[\"img_pixel\"] = cv2.circle(data[\"img_pixel\"], (int(centroid_x), int(centroid_y)), radius=5, color=(255,255,255), thickness=5)\n",
    "            # data[\"img_pixel\"] = cv2.circle(data[\"img_pixel\"], (normalized_px_x,normalized_px_y), radius=5, color=(62,184,64), thickness=3)\n",
    "            data[\"img_pixel\"] = cv2.circle(data[\"img_pixel\"], (int(px_centroid_x),int(px_centroid_y)), radius=5, color=(255,251,28), thickness=5)\n",
    "            #cv2.imwrite(os.path.join(data[\"data_save_path\"])+'/'+str(data[\"frame_index\"])+'_real_.jpg', data[\"img_pixel\"])\n",
    "            # from matplotlib import pyplot as plt\n",
    "            # plt.imshow(data[\"img_pixel\"], interpolation='nearest')\n",
    "            # plt.show()\n",
    "\n",
    "\n",
    "            # agar skala tidak melebihi 1.92\n",
    "            i.x = normalized_px_x / (2 * px_radius) #px_radius nilainya sekitar +-500pixel\n",
    "            i.y = normalized_px_y / (2 * px_radius)\n",
    "            # i.x = px_x / (2 * px_radius) #px_radius nilainya sekitar +-500pixel\n",
    "            # i.y = px_y / (2 * px_radius)\n",
    "            # horizontal_unpix = horizontal / (2 * px_radius)\n",
    "            # vertical_unpix =  vertical / (2 * px_radius)\n",
    "\n",
    "            # print(horizontal_unpix, vertical_unpix)\n",
    "\n",
    "            \n",
    "            ### cek px_radius, px_centroid_x, px_centroid_y, horizontal dan vertical\n",
    "            # print(\"================================\")\n",
    "            # print(data[\"radius\"], data[\"centroid\"], data[\"data_save_path\"])\n",
    "            # print(px_radius, px_centroid_x, px_centroid_y, horizontal, vertical)\n",
    "            # print(normalized_px_x, normalized_px_y, px_x, px_y, i.x, i.y)\n",
    "            #print(str(px_radius))\n",
    "            # i.x = px_x\n",
    "            # i.y = px_y\n",
    "            # print(\"pixel_asli : \"+str([px_x, px_y])+\"pixel_normalized : \"+str([normalized_px_x,normalized_px_y])+\"ix_iy : \"+str([i.x,i.y]))\n",
    "            # hasilnya\n",
    "            # pixel_asli : [723, 927] pixel_normalized : [795, 943] ix_iy : [0.7443820238113403, 0.8829588294029236]\n",
    "            # pixel_asli : [679, 968] pixel_normalized : [751, 984] ix_iy : [0.7031835317611694, 0.9213483333587646]\n",
    "            # pixel_asli : [659, 1017] pixel_normalized : [731, 1033] ix_iy : [0.6844569444656372, 0.9672284722328186]\n",
    "            # pixel_asli : [653, 1054] pixel_normalized : [725, 1070] ix_iy : [0.6788389682769775, 1.0018726587295532]\n",
    "            # pixel_asli : [648, 1081] pixel_normalized : [720, 1097] ix_iy : [0.6741573214530945, 1.0271536111831665]\n",
    "            # pixel_asli : [691, 1041] pixel_normalized : [763, 1057] ix_iy : [0.7144194841384888, 0.9897003769874573]\n",
    "            # pixel_asli : [684, 1088] pixel_normalized : [756, 1104] ix_iy : [0.7078651785850525, 1.033707857131958]\n",
    "            # pixel_asli : [674, 1117] pixel_normalized : [746, 1133] ix_iy : [0.6985018849372864, 1.0608614683151245]\n",
    "            # pixel_asli : [665, 1138] pixel_normalized : [737, 1154] ix_iy : [0.6900749206542969, 1.0805243253707886]\n",
    "            # pixel_asli : [713, 1041] pixel_normalized : [785, 1057] ix_iy : [0.7350187301635742, 0.9897003769874573]\n",
    "            # pixel_asli : [705, 1089] pixel_normalized : [777, 1105] ix_iy : [0.7275280952453613, 1.0346442461013794]\n",
    "            coordinates.append(i.x)\n",
    "            coordinates.append(i.y)\n",
    "            coordinates.append(i.z)\n",
    "            x += 1\n",
    "    else: ### kalo landmarks tidak ditemukan kosongkan saja semua keypoints di tubuh/tangan\n",
    "        if key == \"hand\":\n",
    "            vertex_num = 21\n",
    "        if key == \"pose\":\n",
    "            vertex_num = 33\n",
    "        for i in range(0, vertex_num):\n",
    "            for i in range(0, 3):\n",
    "                coordinates.append(0)\n",
    "\n",
    "    return coordinates, landmarks\n",
    "\n",
    "\n",
    "def draw_landmarks(image, params, key):\n",
    "    mp_drawing = params[\"mp_drawing\"]\n",
    "\n",
    "    # additional codes\n",
    "    # data_save_path = \"./saved_data\"\n",
    "    # if not os.path.exists(data_save_path):\n",
    "    #     os.makedirs(data_save_path)\n",
    "    if key == 'ori':\n",
    "        mp_holistic = mp.solutions.holistic\n",
    "        mp_drawing.draw_landmarks(image, params[\"pose_landmarks\"], mp_holistic.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(\n",
    "                                  color=(0,0,128), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(\n",
    "                                  color=(0,191,255), thickness=2, circle_radius=4)\n",
    "                              )\n",
    "        # cv2.imwrite(os.path.join(params[\"data_save_path\"])+'/'+str(params[\"frame_index\"])+'_original_.jpg', image)\n",
    "        \n",
    "    elif key == 'nor':\n",
    "        mp_holistic = mp.solutions.holistic\n",
    "        mp_drawing.draw_landmarks(image, params[\"normalized\"], mp_holistic.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(\n",
    "                                  color=(250,128,114), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(\n",
    "                                  color=(139,0,0), thickness=2, circle_radius=2)\n",
    "                              )\n",
    "        # cv2.imwrite(os.path.join(params[\"data_save_path\"])+'/'+str(params[\"frame_index\"])+'_normalized_.jpg', image)\n",
    "       \n",
    "    return image\n",
    "\n",
    "def final_extract(params):\n",
    "    result = params[\"result\"]\n",
    "    left_hand_landmarks = result.left_hand_landmarks\n",
    "    right_hand_landmarks = result.right_hand_landmarks\n",
    "    pose_landmarks = params[\"pose_landmarks\"]\n",
    "    shoulders_centroid = params[\"shoulders_centroid\"]\n",
    "    hips_centroid = params[\"hips_centroid\"]\n",
    "    image = params[\"image\"]\n",
    "    im_h = image.shape[0]\n",
    "    im_w = image.shape[1]\n",
    "    # centroid bahu yang direkalulasi piksel resolusi\n",
    "    point_a = pixel_match(shoulders_centroid.copy(), im_w, im_h)\n",
    "    # centroid pinggang yang direkalulasi piksel resolusi\n",
    "    point_b = pixel_match(hips_centroid.copy(), im_w, im_h)\n",
    "   \n",
    "    point_a = np.array(point_a)\n",
    "    point_b = np.array(point_b)\n",
    "    # print(point_a[0],point_b[0])\n",
    "    params[\"image\"] = cv2.line(params[\"image\"], (int(point_a[0]), int(point_a[1])), (int(point_b[0]), int(point_b[1])), color=(255,255,255), thickness=3)\n",
    "    params[\"image\"] = cv2.line(params[\"image\"], (int(shoulders_centroid[0]), int(shoulders_centroid[1])), (int(hips_centroid[0]), int(hips_centroid[1])), color=(255,255,255), thickness=3)\n",
    "\n",
    "    # radius/jarak piksel sesuai dengan rekalkulasi piksel centroid\n",
    "    px_radius = int(euclidean(point_a, point_b))\n",
    "    params[\"px_radius\"] = px_radius\n",
    "    # if params[\"radius\"] == 0:\n",
    "    #     params[\"px_radius_multiplier\"] = 0\n",
    "    # else:\n",
    "    #     # scaling radius dengan keypoints sebelum dan sesudah penyesuaian resolusi frame\n",
    "    #     params[\"px_radius_multiplier\"] = px_radius / params[\"radius\"] \n",
    "    #     # radius rekalkulasi dibagi dengan radius asli untuk dikalikan dengan setiap keypoints di pose terdeteksi\n",
    "\n",
    "    left_hand_coordinates, lh_landmarks = landmarks_data(left_hand_landmarks, params, \"hand\")\n",
    "    right_hand_coordinates, rh_landmarks = landmarks_data(\n",
    "        right_hand_landmarks, params, \"hand\")\n",
    "    pose_coordinates, p_landmarks = landmarks_data(pose_landmarks, params, \"pose\")\n",
    "    coordinates_collection = left_hand_coordinates + \\\n",
    "        right_hand_coordinates + pose_coordinates\n",
    "\n",
    "    params[\"normalized\"] = p_landmarks\n",
    "    draw_landmarks(params[\"image\"],params,\"nor\")\n",
    "\n",
    "    return coordinates_collection\n",
    "\n",
    "def find_centroid(landmarks, data):\n",
    "    indices_a = [11, 12]\n",
    "    indices_b = [23, 24]\n",
    "    centroid_a = np.array(find_body_centroid(landmarks, indices_a))\n",
    "    centroid_b = np.array(find_body_centroid(landmarks, indices_b))\n",
    "    return centroid_a, centroid_b\n",
    "\n",
    "def keypoints_check(data):\n",
    "    # Data\n",
    "    centroid_indices = [0, 11, 12, 23, 24]\n",
    "\n",
    "    ### global bounding box berfungsi untuk menjadi perwakilan titik vektor centroid badan\n",
    "    if data[\"pose_landmarks\"] is None and glob_bbox is None: \n",
    "        ### kalau pose tidak ditemukan dan bounding box belum ada\n",
    "        data[\"centroid\"] = [0, 0, 0]\n",
    "    elif data[\"pose_landmarks\"] is None and glob_bbox is not None: \n",
    "        ### kalau pose tidak ditemukan dan bonding box sudah dibentuk maka yang jadi centroid adalah bounding box frame terakhir\n",
    "        data[\"centroid\"] = glob_bbox\n",
    "    else: #### kalau pose ditemukan\n",
    "        data[\"centroid\"] = find_body_centroid(data[\"pose_landmarks\"], centroid_indices)\n",
    "\n",
    "    if data[\"pose_landmarks\"] is None and glob_dist is not None:\n",
    "        ### kalau pose tidak ditemukan dan jarak kamera sudah terisi\n",
    "        ### digunakan untuk tetap mendapatkan nilai radius walaupun pose tidak terdeteksi. namun wajah atau tangan masih terdeteksi.\n",
    "        # data[\"radius\"] = glob_dist ## digunakan jika ada bantuan deteksi pose oleh tangan atau wajah\n",
    "        data[\"shoulders_centroid\"] = [0, 0, 0]\n",
    "        data[\"hips_centroid\"] = [0, 0, 0]\n",
    "    else: ### kalau pose ditemukan\n",
    "        centroid_a, centroid_b = find_centroid(data[\"pose_landmarks\"], data) ## dapatkan centroid bahu dan centroid pinggang\n",
    "        data[\"radius\"] = euclidean(centroid_a, centroid_b) # radius jarak objek terhadap bounding box\n",
    "        data[\"shoulders_centroid\"] = centroid_a ## centroid bahu\n",
    "        data[\"hips_centroid\"] = centroid_b ## centroid pinggang\n",
    "    return data    \n",
    "\n",
    "\n",
    "def normalize(holistic, mp_holistic, image):\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # image.flags.writeable = False   \n",
    "    result = holistic.process(image)  # original result\n",
    "    original = []\n",
    "    \n",
    "    params = {\"pose_landmarks\": result.pose_landmarks,\n",
    "              \"image\": image, \n",
    "              \"result\": result, \n",
    "              \"mp_drawing\": mp_drawing, \n",
    "              # \"frame_index\" : frame_index,\n",
    "              # \"data_save_path\" : save_path\n",
    "              }\n",
    "    \n",
    "    if result.left_hand_landmarks:\n",
    "        for res in result.left_hand_landmarks.landmark:\n",
    "            original.append(res.x)\n",
    "            original.append(res.y)\n",
    "            original.append(res.z)\n",
    "    else:\n",
    "        for i in range(0, 21):\n",
    "            for i in range(0, 3):\n",
    "                original.append(0)\n",
    "    \n",
    "    if result.right_hand_landmarks:\n",
    "        for res in result.right_hand_landmarks.landmark:\n",
    "            original.append(res.x)\n",
    "            original.append(res.y)\n",
    "            original.append(res.z)\n",
    "    else:\n",
    "        for i in range(0, 21):\n",
    "            for i in range(0, 3):\n",
    "                original.append(0)\n",
    "\n",
    "    if result.pose_landmarks:\n",
    "        for res in result.pose_landmarks.landmark:\n",
    "            original.append(res.x)\n",
    "            original.append(res.y)\n",
    "            original.append(res.z)\n",
    "    else:\n",
    "        for i in range(0, 33):\n",
    "            for i in range(0, 3):\n",
    "                original.append(0)\n",
    "        \n",
    "    #print(original)\n",
    "\n",
    "    # o = save_path+\"/\"+str(frame_index)+\"_original\"\n",
    "    # np.savetxt(o+\".csv\",original,delimiter=\",\")\n",
    "    params = keypoints_check(params)\n",
    "    draw_landmarks(image,params,\"ori\")\n",
    "    # image.flags.writeable = True   \n",
    "    # image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    coordinates = final_extract(params)\n",
    "  \n",
    "\n",
    "    return coordinates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d17bce93-8be6-467a-ba47-29b4fc98f5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_transformer_model(sequence_length, num_features, num_classes, num_layers, hidden_units, num_heads, dropout_rate):\n",
    "    # Input Layer\n",
    "    inputs = layers.Input(shape=(sequence_length, num_features))\n",
    "    x = inputs\n",
    "\n",
    "    # Positional Encoding Layer\n",
    "    positional_encoding = layers.Embedding(input_dim=sequence_length, output_dim=num_features)(tf.range(sequence_length))\n",
    "    x += positional_encoding\n",
    "\n",
    "    # Encoder Layers\n",
    "    for _ in range(num_layers):\n",
    "        # Multi-Head Attention\n",
    "        attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=num_features)(x, x)\n",
    "        attention = layers.Dropout(rate=dropout_rate)(attention)\n",
    "        attention = layers.LayerNormalization(epsilon=1e-6)(attention + x)\n",
    "\n",
    "        # Feed Forward Neural Network\n",
    "        ffn = layers.Dense(units=hidden_units, activation='relu')(attention)\n",
    "        ffn = layers.Dropout(rate=dropout_rate)(ffn)\n",
    "        ffn = layers.LayerNormalization(epsilon=1e-6)(ffn + attention)\n",
    "\n",
    "        x = ffn\n",
    "\n",
    "    # Global Average Pooling\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Output Layer\n",
    "    outputs = layers.Dense(units=num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Create and compile the model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66917dad-3341-455a-825a-b6882c6b7ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions that we try to detect\n",
    "actions = np.array(['Akan', 'Anda', 'Apa', 'Atau', 'Baca', 'Bagaimana', 'Bahwa', 'Beberapa', 'Besar',\n",
    "    'Bisa', 'Buah', 'Dan', 'Dari', 'Dengan', 'Dia', 'Haus', 'Ingin', 'Ini', 'Itu',\n",
    "    'Jadi', 'Juga', 'Kami', 'Kata', 'Kecil', 'Kumpul', 'Labuh', 'Lain', 'Laku',\n",
    "    'Lapar', 'Main', 'Makan', 'Masing', 'Mereka', 'Milik', 'Minum', 'Oleh', 'Pada',\n",
    "    'Rumah', 'Satu', 'Saya', 'Sebagai', 'Tambah', 'Tangan', 'Tetapi', 'Tidak', 'Tiga',\n",
    "    'Udara', 'Untuk', 'Waktu', 'Yang'])\n",
    "\n",
    "selected_actions = np.array(['Anda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3046ec3c-c66a-4f1f-a281-6f324308f284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 225)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_array = np.zeros((120,225))\n",
    "np.shape(init_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b62fb890-53c2-4564-b918-3b7b63a9559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_transformer_model(init_array.shape[0],init_array.shape[1],50,2,225,4,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f4c15d4-ad60-48d4-b76e-4416c806d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('Tr_FullData_Layer_50_Epoch_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f67b3bef-3499-4091-8da6-cd629dca39d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def shuffle_soal():\n",
    "    random_soal = random.choice(selected_actions)\n",
    "    return random_soal\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "670e09f0-1f0a-4ab6-a646-70a0903190ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Make detections\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m coordinates \u001b[38;5;241m=\u001b[39m normalize(holistic, mp_holistic, frame)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# NEW Apply wait logic\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame_num \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n",
      "Cell \u001b[1;32mIn [3], line 232\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(holistic, mp_holistic, image)\u001b[0m\n\u001b[0;32m    229\u001b[0m mp_drawing \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mdrawing_utils\n\u001b[0;32m    230\u001b[0m \u001b[38;5;66;03m# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m# image.flags.writeable = False   \u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mholistic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# original result\u001b[39;00m\n\u001b[0;32m    233\u001b[0m original \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    235\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpose_landmarks\u001b[39m\u001b[38;5;124m\"\u001b[39m: result\u001b[38;5;241m.\u001b[39mpose_landmarks,\n\u001b[0;32m    236\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: image, \n\u001b[0;32m    237\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m: result, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    240\u001b[0m           \u001b[38;5;66;03m# \"data_save_path\" : save_path\u001b[39;00m\n\u001b[0;32m    241\u001b[0m           }\n",
      "File \u001b[1;32mD:\\Python\\lib\\site-packages\\mediapipe\\python\\solutions\\holistic.py:160\u001b[0m, in \u001b[0;36mHolistic.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    137\u001b[0m   \u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks, left and right hand landmarks, and face landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Python\\lib\\site-packages\\mediapipe\\python\\solution_base.py:353\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    347\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    348\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionBase can only process non-audio and non-proto-list data. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    349\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_stream_type_info[stream_name]\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    350\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype is not supported yet.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (input_stream_type \u001b[38;5;241m==\u001b[39m PacketDataType\u001b[38;5;241m.\u001b[39mIMAGE_FRAME \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    352\u001b[0m       input_stream_type \u001b[38;5;241m==\u001b[39m PacketDataType\u001b[38;5;241m.\u001b[39mIMAGE):\n\u001b[1;32m--> 353\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m!=\u001b[39m RGB_CHANNELS:\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput image must contain three channel rgb data.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    355\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    356\u001b[0m       stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    357\u001b[0m       packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    358\u001b[0m                                data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "soal = []\n",
    "threshold = 0.5\n",
    "sequence_length = 121\n",
    "\n",
    "new_soal = shuffle_soal()\n",
    "soal.append(new_soal)\n",
    "frame_counter = 0  # Initialize the frame counter variable\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.6, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    for frame_num in range(sequence_length):\n",
    "\n",
    "        # Read feed\n",
    "                    ret, frame = cap.read()\n",
    "\n",
    "                    # Make detections\n",
    "                    coordinates = normalize(holistic, mp_holistic, frame)\n",
    "\n",
    "\n",
    "                    # NEW Apply wait logic\n",
    "                    if frame_num == 0: \n",
    "                        cv2.rectangle(frame, (0,0), (800, 40), (255,255,255), -1)\n",
    "                        text = 'BERSIAP'\n",
    "                        x, y = 250, 250\n",
    "                        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                        font_scale = 1\n",
    "                        font_thickness = 4\n",
    "                        outline_thickness = 8\n",
    "                        text_color = (0, 0, 0)  # Black color\n",
    "                        outline_color = (255, 255, 255)  # White color\n",
    "\n",
    "                        # Draw the text outline\n",
    "                        (text_width, text_height) = cv2.getTextSize(text, font, font_scale, font_thickness)[0]\n",
    "                        cv2.putText(frame, text, (x, y), font, font_scale, outline_color, outline_thickness, cv2.LINE_AA)\n",
    "\n",
    "                        # Draw the main text\n",
    "                        cv2.putText(frame, text, (x, y), font, font_scale, text_color, font_thickness, cv2.LINE_AA)\n",
    "                        # Show to screen\n",
    "                        cv2.imshow('OpenCV Feed', cv2.resize(frame, (800, 600)))\n",
    "                        cv2.waitKey(2500)\n",
    "                    else: \n",
    "                        cv2.rectangle(frame, (0,0), (800, 40), (255,255,255), -1)\n",
    "                        cv2.putText(frame, 'Soal :' ,(200,30), \n",
    "                                       cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0,0,0), 1, cv2.LINE_AA)\n",
    "                        cv2.putText(frame, ''.join(soal[-1:]), (300,30), \n",
    "                                       cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0,0,0), 1, cv2.LINE_AA)\n",
    "                        sequence.append(coordinates)\n",
    "                        cv2.imshow('OpenCV Feed', cv2.resize(frame, (800, 600)))\n",
    "                        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    # Open a new window after the main loop\n",
    "    #Predict and Detect\n",
    "    if len(sequence) == 120:\n",
    "        res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "        print(actions[np.argmax(res)])\n",
    "        predictions.append(np.argmax(res))\n",
    "            \n",
    "    #3. Viz logic\n",
    "        if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "            if res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "        if len(sentence) > 1: \n",
    "            sentence = sentence[-1:]\n",
    "    new_window = np.ones((480, 640, 3), dtype=np.uint8) * 255  # Create a white image\n",
    "    if sentence[-1:] == soal[-1:]:\n",
    "        cv2.putText(new_window, 'Evaluasi : Benar' ,(250,270), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    elif sentence[-1:] != soal[-1:]:\n",
    "        cv2.putText(new_window, 'Evaluasi : Salah' ,(250,270), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(new_window, 'Soal :' ,(250,230), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0,0,0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(new_window, 'Prediksi :' ,(250,250), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0,0,0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(new_window, ''.join(soal[-1:]), (325,230), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0,0,0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(new_window, ''.join(sentence), (375,250), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0,0,0), 1, cv2.LINE_AA)\n",
    "    cv2.imshow('New Window', cv2.resize(new_window, (800, 600)))\n",
    "    cv2.waitKey(0)  # Wait until a key is pressed\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6e7ae5e-e2cc-41bf-84b2-602fa21cd28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "soal = []\n",
    "threshold = 0.5\n",
    "sequence_length = 121\n",
    "\n",
    "# new_soal = shuffle_soal()\n",
    "# soal.append(new_soal)\n",
    "frame_counter = 0  # Initialize the frame counter variable\n",
    "\n",
    "cap = cv2.VideoCapture('E:/dataset/video/dia/1.mp4')\n",
    "\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.6, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Make detections\n",
    "        coordinates = normalize(holistic, mp_holistic, frame)\n",
    "        sequence.append(coordinates)\n",
    "        cv2.imshow('OpenCV Feed', cv2.resize(frame, (1366, 768)))\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    # Predict and Detect\n",
    "    if len(sequence) == 120:\n",
    "        res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "        print(actions[np.argmax(res)])\n",
    "        predictions.append(np.argmax(res))\n",
    "            \n",
    "    #3. Viz logic\n",
    "        if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "            if res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "        if len(sentence) > 1: \n",
    "            sentence = sentence[-1:]\n",
    "    new_window = np.ones((480, 640, 3), dtype=np.uint8) * 255  # Create a white image\n",
    "#     if sentence[-1:] == soal[-1:]:\n",
    "#         cv2.putText(new_window, 'Evaluasi : Benar' ,(250,270), \n",
    "#                     cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "#     elif sentence[-1:] != soal[-1:]:\n",
    "#         cv2.putText(new_window, 'Evaluasi : Salah' ,(250,270), \n",
    "#                     cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "    # cv2.putText(new_window, 'Soal :' ,(250,230), \n",
    "    #                     cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0,0,0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(new_window, 'Prediksi :' ,(250,250), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0,0,0), 1, cv2.LINE_AA)\n",
    "    # cv2.putText(new_window, ''.join(soal[-1:]), (325,230), \n",
    "    #                     cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0,0,0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(new_window, ''.join(sentence), (375,250), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0,0,0), 1, cv2.LINE_AA)\n",
    "    cv2.imshow('New Window', cv2.resize(new_window, (800, 600)))\n",
    "    cv2.waitKey(0)  # Wait until a key is pressed\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89507371-01f7-4fbc-9845-8b0ac4aeabb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n",
      "Tangan\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "print(actions[np.argmax(res)])\n",
    "predictions.append(np.argmax(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ed677f4-efc2-42e6-9446-d1ad57bfe6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[-1:] == soal[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4715c480-91b3-4eaf-91ee-ef476f663e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salah\n"
     ]
    }
   ],
   "source": [
    "    if sentence[-1:] == soal[-1:]:\n",
    "        print('Benar')\n",
    "\n",
    "    elif sentence[-1:] != soal[-1:]:\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca83ebfa-632b-49d7-ad85-1ae9b37f0b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Untuk']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soal[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "250949ed-7468-4ed4-9f16-c17b6f4ec82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d8eba4-46e8-446b-8be2-d703da55774b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "39a65861-2a6a-4c0f-a383-276216c592d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "soal = []\n",
    "threshold = 0.5\n",
    "\n",
    "new_soal = shuffle_soal()\n",
    "soal.append(new_soal)\n",
    "frame_counter = 0  # Initialize the frame counter variable\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.6, min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Show to screen\n",
    "        coordinates = normalize(holistic, mp_holistic, frame) \n",
    "        sequence.append(coordinates)\n",
    "        sequence = sequence[-120:]\n",
    "        \n",
    "        #Predict and Detect\n",
    "        if len(sequence) == 120:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "        #3. Viz logic\n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 1: \n",
    "                sentence = sentence[-1:]\n",
    "\n",
    "\n",
    "            \n",
    "        cv2.rectangle(frame, (0,0), (200, 40), (255,255,255), -1)\n",
    "        cv2.rectangle(frame, (200,0), (400, 40), (0,0,0), -1)\n",
    "        cv2.rectangle(frame, (400,0), (800, 40), (255,255,255), -1)\n",
    "        \n",
    "        if sentence[-1:] == soal[-1:]:\n",
    "                cv2.putText(frame, 'BENAR' ,(250,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.65, (255,255,255), 2, cv2.LINE_AA)\n",
    "                if frame_counter >= 20:  # Check if the display has been shown for 30 frames\n",
    "                    while soal[-1] == new_soal:\n",
    "                        new_soal = shuffle_soal()\n",
    "                    soal.append(new_soal)\n",
    "                    frame_counter = 0  # Reset the frame counter\n",
    "                \n",
    "        elif sentence[-1:] != soal[-1:]:\n",
    "                cv2.putText(frame, 'SALAH' ,(250,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.65, (255,255,255), 2, cv2.LINE_AA)\n",
    "                \n",
    "        cv2.putText(frame, 'Predicted :' ,(3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(frame, 'Soal :' ,(420,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(frame, ''.join(sentence), (125,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0,0,0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(frame, ''.join(soal[-1:]), (495,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0,0,0), 1, cv2.LINE_AA)\n",
    "        \n",
    "        \n",
    "        cv2.imshow('OpenCV Feed', cv2.resize(frame, (800, 600)))\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1406dfe7-25f7-4b97-81eb-94a50bb02a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a18e9bc9-3481-4b2a-aa7d-b82ded4358a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'model/tf.__operators__.add/AddV2' defined at (most recent call last):\n    File \"D:\\Python\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"D:\\Python\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"D:\\Python\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"D:\\Python\\lib\\site-packages\\traitlets\\config\\application.py\", line 978, in launch_instance\n      app.start()\n    File \"D:\\Python\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"D:\\Python\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"D:\\Python\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n      self._run_once()\n    File \"D:\\Python\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n      handle._run()\n    File \"D:\\Python\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"D:\\Python\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"D:\\Python\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"D:\\Python\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"D:\\Python\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"D:\\Python\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"D:\\Python\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"D:\\Python\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2885, in run_cell\n      result = self._run_cell(\n    File \"D:\\Python\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n      return runner(coro)\n    File \"D:\\Python\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"D:\\Python\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3139, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"D:\\Python\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3318, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"D:\\Python\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3378, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12188\\2543061522.py\", line 1, in <module>\n      res = model.predict(np.expand_dims(sequence, axis=0))[0]\n    File \"D:\\Python\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Python\\lib\\site-packages\\keras\\engine\\training.py\", line 2253, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"D:\\Python\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function\n      return step_function(self, iterator)\n    File \"D:\\Python\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\Python\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step\n      outputs = model.predict_step(data)\n    File \"D:\\Python\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n      return self(x, training=False)\n    File \"D:\\Python\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Python\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"D:\\Python\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Python\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\Python\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Python\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"D:\\Python\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"D:\\Python\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Python\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\Python\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Python\\lib\\site-packages\\keras\\layers\\core\\tf_op_layer.py\", line 242, in _call_wrapper\n      return self._call_wrapper(*args, **kwargs)\n    File \"D:\\Python\\lib\\site-packages\\keras\\layers\\core\\tf_op_layer.py\", line 279, in _call_wrapper\n      result = self.function(*args, **kwargs)\nNode: 'model/tf.__operators__.add/AddV2'\nIncompatible shapes: [1,35,225] vs. [120,225]\n\t [[{{node model/tf.__operators__.add/AddV2}}]] [Op:__inference_predict_function_743]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39mexpand_dims(sequence, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(actions[np\u001b[38;5;241m.\u001b[39margmax(res)])\n",
      "File \u001b[1;32mD:\\Python\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'model/tf.__operators__.add/AddV2' defined at (most recent call last):\n    File \"D:\\Python\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"D:\\Python\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"D:\\Python\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"D:\\Python\\lib\\site-packages\\traitlets\\config\\application.py\", line 978, in launch_instance\n      app.start()\n    File \"D:\\Python\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"D:\\Python\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"D:\\Python\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n      self._run_once()\n    File \"D:\\Python\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n      handle._run()\n    File \"D:\\Python\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"D:\\Python\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"D:\\Python\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"D:\\Python\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"D:\\Python\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"D:\\Python\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"D:\\Python\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"D:\\Python\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2885, in run_cell\n      result = self._run_cell(\n    File \"D:\\Python\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n      return runner(coro)\n    File \"D:\\Python\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"D:\\Python\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3139, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"D:\\Python\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3318, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"D:\\Python\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3378, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12188\\2543061522.py\", line 1, in <module>\n      res = model.predict(np.expand_dims(sequence, axis=0))[0]\n    File \"D:\\Python\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Python\\lib\\site-packages\\keras\\engine\\training.py\", line 2253, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"D:\\Python\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function\n      return step_function(self, iterator)\n    File \"D:\\Python\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\Python\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step\n      outputs = model.predict_step(data)\n    File \"D:\\Python\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n      return self(x, training=False)\n    File \"D:\\Python\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Python\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"D:\\Python\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Python\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\Python\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Python\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"D:\\Python\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"D:\\Python\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Python\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\Python\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Python\\lib\\site-packages\\keras\\layers\\core\\tf_op_layer.py\", line 242, in _call_wrapper\n      return self._call_wrapper(*args, **kwargs)\n    File \"D:\\Python\\lib\\site-packages\\keras\\layers\\core\\tf_op_layer.py\", line 279, in _call_wrapper\n      result = self.function(*args, **kwargs)\nNode: 'model/tf.__operators__.add/AddV2'\nIncompatible shapes: [1,35,225] vs. [120,225]\n\t [[{{node model/tf.__operators__.add/AddV2}}]] [Op:__inference_predict_function_743]"
     ]
    }
   ],
   "source": [
    "res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "print(actions[np.argmax(res)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "41e8c3f9-7476-49ff-9fd5-f700871fef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_soal = shuffle_soal()\n",
    "soal.append(new_soal)\n",
    "\n",
    "for _ in range(4):\n",
    "    while soal[-1] == new_soal:\n",
    "        new_soal = shuffle_soal()\n",
    "\n",
    "soal.append(new_soal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "210d1e85-1e45-4736-ba3a-1e1f0f9819ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dia']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e0a0a17f-e9ad-4610-a04d-2a75518c0686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soal[-1] == new_soal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9cd88892-aa14-424e-a548-3bf5a2b37a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Untuk'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_soal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e39cd80f-9400-463c-ac93-eb993e2163cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_soal == sentence[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
